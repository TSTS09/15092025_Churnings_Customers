# -*- coding: utf-8 -*-
"""Tchio Sekale_Assignment 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19Hx0104Z_aLtAerLadjqYdyFGb7rFIJa
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/CustomerChurn_dataset.csv')
data.drop('customerID', axis=1, inplace = True)# removing CustomerID from dataset
data_2 = data.copy()

"""#Initial EDA"""

# Clean the column by replacing empty strings with NaN
data['TotalCharges'] = data['TotalCharges'].replace(' ', pd.NA)

# Convert the column to float, ignoring NaN values
data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')

data.info()

data.head()

data.nunique()

data.isnull().sum() # getting the number of missing values in each columns

((data.isnull().sum())/len(data))*100 # getting the percentage of missing values in each  columns

"""Multiple imputation for the totalcharges column"""

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

imputer = IterativeImputer(max_iter=10, random_state=0)
data['TotalCharges'] = imputer.fit_transform(data[['TotalCharges']])

data.describe(include = 'all') # getting an indepth statistical description of the data

data.dtypes # understanding the datatypes of each column

"""#Feature Extraction

Splitting numerical and categorical varibales
"""

numerical_columns = data.select_dtypes(include=['int64', 'float64']).columns
categorical_columns = data.select_dtypes(include=['object']).columns

Numerical_var = data[numerical_columns]
Categorical_var = data[categorical_columns]

Categorical_var

"""Encoding categorical data"""

columns_to_encode = Categorical_var.loc[:, 'gender':'Churn']
Categorical_var[columns_to_encode.columns] = columns_to_encode.apply(lambda x:pd.factorize(x)[0])
final_categorical = pd.DataFrame(Categorical_var[columns_to_encode.columns])

final_categorical

"""Joining back the preprocessed categorical and numerical variables"""

clean_data = pd.concat([Numerical_var,final_categorical], axis=1)
clean_data

y = clean_data['Churn']
clean_data.drop('Churn', axis=1, inplace = True)

#Using RandomForestClassifier
random = RandomForestClassifier()
random.fit(clean_data, y)  # Train the model

# Access feature importances
feature_importances = random.feature_importances_

#Visualizing the feature importance
plt.figure(figsize=(10, 6))
plt.bar(range(len(feature_importances)), feature_importances, tick_label=clean_data.columns)
plt.xlabel('Features')
plt.ylabel('Feature Importance')
plt.title('Feature Importance in RandomForest Model')
plt.xticks(rotation='vertical')
plt.show()

#Ranking the features based on the results above
feature_names = clean_data.columns

# Sort feature importance in descending order
indices = np.argsort(feature_importances)[::-1]

# Print feature ranking
print("Feature ranking based on Random Forest Classifier:")
for f in range(len(feature_names)):
    print(f"{f + 1}. {feature_names[indices[f]]}: {feature_importances[indices[f]]}")

# Create a DataFrame with feature importances and corresponding column names
importance_df = pd.DataFrame({'Feature': clean_data.columns, 'Importance': feature_importances})

# Sort the DataFrame by importance in descending order
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Select the top 35 feature names
top_15_feature_names = importance_df['Feature'].head(15).tolist()

# Select the columns from clean_data that match the top feature names
feature_subset = clean_data[top_15_feature_names]

# Display the resulting DataFrame with the selected columns
feature_subset

from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_regression
import pandas as pd

# Perform feature selection to determine the best features (e.g., using SelectKBest with f_regression)
k_best = SelectKBest(score_func=f_regression, k=10)
best_features = k_best.fit_transform(feature_subset, y)

# Get the column names of the best features
best_feature_indices = k_best.get_support(indices=True)
best_feature_names = feature_subset.columns[best_feature_indices]

# Initialize PCA without specifying the number of components
pca = PCA()

# Fit the PCA model to your data
pca.fit(best_features)

# Get the explained variance of each principal component
explained_variance = pca.explained_variance_ratio_

# Sort the feature names based on the explained variance (highest variance first)
sorted_feature_names = [name for _, name in sorted(zip(explained_variance, best_feature_names), reverse=True)]

# Select the top N feature names
top_n = 7
selected_feature_names = sorted_feature_names[:top_n]
newX = feature_subset[selected_feature_names]
newX

explained_variance = pca.explained_variance_ratio_
print(f"Explained Variance: {explained_variance.sum() * 100:.2f}%")

"""### Standardising the independent variables and inputting the proper column names and formats"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

scaled_features=scaler.fit_transform(newX)

scaled_df = pd.DataFrame(scaled_features, columns=newX.columns)
X= scaled_df

X

"""#Exploratory Data Analysis(EDA)"""

data2= data[['TotalCharges', 	'MonthlyCharges', 	'tenure', 	'Contract', 	'PaymentMethod', 	'OnlineSecurity', 	'TechSupport', 'Churn']]

# Visualize the distribution of the target variable 'Churn'
sns.countplot(x='Churn', data=data2)
plt.title('Churn Distribution')
plt.show()

"""Output: About 28% of customer eventually churn over a given amount of time after using the proucts."""

# Explore numerical variables against churn
fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 5))

sns.boxplot(x='Churn', y='TotalCharges', data=data2, ax=axes[0])
axes[0].set_title('Churn by TotalCharges')

sns.boxplot(x='Churn', y='MonthlyCharges', data=data2, ax=axes[1])
axes[1].set_title('Churn by MonthlyCharges')

sns.boxplot(x='Churn', y='tenure', data=data2, ax=axes[2])
axes[2].set_title('Churn by Tenure')

plt.tight_layout()
plt.show()

"""Output= Based on the numerical data = an individual with a total charge of about 1000, Monthly charge of about 80 and a tenuure time of 10 years or less is likely to churn."""

# Explore categorical variables against churn
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 10))

sns.countplot(x='Contract', hue='Churn', data=data2, ax=axes[0, 0])
axes[0, 0].set_title('Churn by Contract')

sns.countplot(x='PaymentMethod', hue='Churn', data=data2, ax=axes[0, 1])
axes[0, 1].set_title('Churn by PaymentMethod')

sns.countplot(x='OnlineSecurity', hue='Churn', data=data2, ax=axes[1, 0])
axes[1, 0].set_title('Churn by OnlineSecurity')

sns.countplot(x='TechSupport', hue='Churn', data=data2, ax=axes[1, 1])
axes[1, 1].set_title('Churn by TechSupport')

plt.tight_layout()
plt.show()

"""Output: Based on the categorical data. Below is th eprofiel of a customer likely to churn:
An individual with a Month-to-Month contract, using a electronic check as payment method, who is neither using online security tools nor tech support.
"""

new_data = pd.concat([y,newX], axis=1)
plt.figure(figsize=(13,17))
sns.pairplot(data=new_data.drop(['Churn'],axis=1))
plt.show()

"""Insights:
- The variable Total charge has a positive correlation with Monthly Charge and tenure.Thus, As Monthly charge and/or ternure increase, total charge increases leading to a reducing chruning possibility.
- There is also a positive correlation between TechSupport and OnlineSecurity. Thus, as customers request for technical support, there is a high probability they will subscribe to online security tools.
"""

plt.figure(figsize=(12, 7))
sns.heatmap((new_data).corr(), annot = True, vmin = -1, vmax = 1)
plt.show()

"""Output:
- Strong positive correlation between online security and TechSupport (0.79):
        
Explanation: This suggests that as the level of online security increases, the level of TechSupport also tends to increase. If one invests more in online security measures, there's a strong tendency to also invest more in TechSupport.
- Mild positive correlation between Total charge and Monthly charge (0.65):

Explanation: This implies that as the total charges for a service increase, the monthly charges also tend to increase, but the relationship is not as strong as a perfect positive correlation. Changes in total charges are moderately associated with changes in monthly charges.
- Strong positive correlation between Total charge and tenure (0.83):

Explanation: This indicates that as the tenure of a customer (how long they have been using the service) increases, the total charges also tend to increase significantly. Longer-term customers tend to accumulate higher total charges.
- Mild positive correlation between tenure and contract (0.67):

Explanation: As the tenure of a customer increases, there's a mild tendency for the type of contract (perhaps, longer-term contracts) to also increase. However, the relationship is not as strong as in the previous example.

- Mild negative correlation between Monthly Charges with TechSupport (0.6) and OnlineSecurity (0.62):


Explanation: Here, as monthly charges increase, there's a mild tendency for TechSupport and OnlineSecurity to decrease. This suggests that customers who pay higher monthly charges are less likely to invest in additional services like TechSupport and OnlineSecurity.

##Profile of customer related to churning

Sample Customer Churn Profile:
**Note: Name, Age, occupation, location and Gender are not relevant for churning. They were added to personify someone who has the high probability of churning**

Name: Alex Johnson

Age: 42

Occupation: Sales Representative

Location: Suburban area, West

Gender: Male

Usage Characteristics of customer likely to churn:

  - Monthly Charge: $85

  - Total Charge: $950
  - Tenure: 7 years
  - Contract Type: Month-to-Month
  - Payment Method: Electronic Check

Service Usage:

  - Online Security: Not Subscribed
  - Tech Support: Not Subscribed

Behavioral Patterns:

  - Over the past 7 years, Alex's total charges have seen a noticeable increase, possibly indicating a growing reliance on the service.
  - Preferring the flexibility of a Month-to-Month contract, Alex values the freedom to adapt their plan as needed.
  - The use of electronic check payments suggests a preference for convenient and automated transactions.
  - While there is a positive correlation between total charge and tenure, Alex's tenure falls short of the 10-year mark associated with a higher probability of churning.
  - The absence of subscriptions to online security and tech support aligns with the categorical data indicating a propensity to churn for customers with this profile.

Correlation Analysis:

  A mild negative correlation between Alex's monthly charges and the likelihood to invest in additional services like Tech Support and Online Security is observed. As monthly charges increase, the probability of subscribing to these services tends to decrease, in line with the general trend identified in the dataset.

Prediction:
Considering Alex's usage characteristics and the identified correlations, there is a moderate likelihood of churn. The Month-to-Month contract, electronic check payment method, and the absence of online security and tech support subscriptions contribute to this prediction. Implementing retention strategies, such as promoting longer-term contracts or personalized offers on additional services, may be effective in retaining Alex as a customer.

#Training
"""

!pip install scikeras

from scikeras.wrappers import KerasClassifier
#from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold
from sklearn.metrics import roc_auc_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""Training MLP model using functional API"""

# define a funuction to create the MLP model
def mlp_model(hidden_units=64, optimizers = 'adam'):
  input_layer = Input(shape=(X_train.shape[1],))
  hidden_layer1 = Dense(hidden_units, activation = 'relu')(input_layer)
  hidden_layer2 = Dense(hidden_units // 2, activation = 'tanh')(hidden_layer1)
  hidden_layer3 = Dense(hidden_units // 4, activation = 'elu')(hidden_layer2)
  output_layer = Dense(1, activation = 'sigmoid' )(hidden_layer3)
  model = Model(inputs = input_layer, outputs = output_layer)
  model.compile(optimizer = optimizers, loss ='binary_crossentropy', metrics = ['accuracy'])
  return model

k_model = KerasClassifier(build_fn=mlp_model, epochs= 5, hidden_units = 32, verbose =1)

# Define the hyperparameters and values to search over in the grid search
param_grid = {
    'hidden_units': [ 16 ,32, 64],
    'batch_size' : [20, 32, 64],
    'epochs':[20, 40, 60, 80],
}

# Create a GridSearchCV instance with cross-validation
cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
grid = GridSearchCV(estimator=k_model, param_grid=param_grid, cv=cv, scoring='accuracy', verbose=1)

# Perform the grid search on the training data
grid_result = grid.fit(X_train, y_train)

# Get the best parameters and the corresponding accuracy
print(f"Best Parameters: {grid_result.best_params_}")
print(f"Best Accuracy: {grid_result.best_score_}")

# Get the best model
best_model = grid_result.best_estimator_

# Predict probabilities on the test set
y_pred_proba = best_model.predict_proba(X_test)

# Calculate AUC score
auc_score = roc_auc_score(y_test, y_pred_proba[:, 1])
print(f"AUC Score: {auc_score}")

"""#Optimising, testing and training"""

# split the train data into train and validation
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

optimized_model = mlp_model(hidden_units=grid_result.best_params_["hidden_units"], optimizers=grid_result.best_estimator_.optimizer)
history = optimized_model.fit(X_train, y_train, batch_size=grid_result.best_params_["batch_size"], epochs=grid_result.best_params_["epochs"], verbose=2, validation_data=(X_val, y_val))

# Evaluate the data with the test set
test_loss, test_accuracy = optimized_model.evaluate(X_test, y_test)
print("Model Test Accuracy: ", str(test_accuracy))

epochs = history.epoch
history = history.history

# Visualize the train and test losses
plt.title("Training Loss vs Validation Loss")
plt.plot(epochs, history["loss"], label="Train Loss")
plt.plot(epochs, history["val_loss"], label="Val Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()

# Visualize Accuracy for train and validation sets
plt.title("Training Accuracy vs Validation Accuracy")
plt.plot(epochs, history["accuracy"], label="Train Accuracy")
plt.plot(epochs, history["val_accuracy"], label="Val Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()

# check auc score
y_pred = optimized_model.predict(X_test)
print(" model auc score:", roc_auc_score(y_test, y_pred))

"""# Deployment"""

# Saving the model
optimized_model.save("churn_model.h5")
import pickle
# Saving the standard scaler
with open('scaler.pkl', 'wb') as file:
  pickle.dump(scaler, file)